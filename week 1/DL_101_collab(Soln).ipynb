{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Let build a neuron**"
      ],
      "metadata": {
        "id": "ErS90aXeQe7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.weights = [.5] * num_inputs\n",
        "        self.bias = 0\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    \n",
        "    def activate(self, inputs):\n",
        "        \n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        print(weighted_sum)\n",
        "        \n",
        "        # Apply the activation function (in this case, sigmoid)\n",
        "        activation = self.sigmoid(weighted_sum)\n",
        "        \n",
        "        return activation\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "FpHfWa-ZQlHz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = Neuron(3)\n",
        "inputs = [0.5, 0.7, 2]\n",
        "output = neuron.activate(inputs)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx75EUG9SCXZ",
        "outputId": "7bad93b9-4071-4a21-a0ae-84754f4d0ba4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6\n",
            "0.8320183851339245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the input number of input to 6 and use another activation function named ReLu**"
      ],
      "metadata": {
        "id": "pRdrLLj-UfE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.weights = [.5] * num_inputs\n",
        "        self.bias = 0\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    \n",
        "    def Relu(self,x):\n",
        "        if x>0:\n",
        "          return x\n",
        "        else:\n",
        "          return 0 \n",
        "    \n",
        "    def activate(self, inputs):\n",
        "        \n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        print(weighted_sum)\n",
        "        \n",
        "        # Apply the activation function (in this case, sigmoid)\n",
        "        activation = self.Relu(weighted_sum)\n",
        "        \n",
        "        return activation"
      ],
      "metadata": {
        "id": "21PaI954SMLM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = Neuron(3)\n",
        "inputs = [0.5, 0.7, 2]\n",
        "output = neuron.activate(inputs)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqeKqGcYSaha",
        "outputId": "58b0a3ee-81ab-4ab3-ea5e-43139fb431f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6\n",
            "1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember we learn to generate random number**"
      ],
      "metadata": {
        "id": "4VSqJbqNWIbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "np.random.random(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVkg1LldVpFS",
        "outputId": "4840f7b9-1140-4e0e-8031-3e733cedf9e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.33482874, 0.37995213])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now  \n",
        "\n",
        "1.   instead of prdefined weight initiate with random weight\n",
        "2.   Instead of giving the class the number of input give the class input directly\n",
        "3.   Add option to select the prefered activation funtion \n"
      ],
      "metadata": {
        "id": "JMg222REWR0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, inputs):\n",
        "        self.input=inputs\n",
        "        self.weights = np.random.random(len(inputs))\n",
        "        self.bias = np.random.random(1)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    \n",
        "    def Relu(self,x):\n",
        "        if x>0:\n",
        "          return x\n",
        "        else:\n",
        "          return 0 \n",
        "    \n",
        "    def activate(self):\n",
        "        \n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = np.dot(self.input, self.weights) + self.bias\n",
        "        print(weighted_sum)\n",
        "        \n",
        "        # Apply the activation function (in this case, sigmoid)\n",
        "        activation = self.Relu(weighted_sum)\n",
        "        \n",
        "        return activation"
      ],
      "metadata": {
        "id": "gadjHI8cVzgt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [0.5, 0.7, 2]\n",
        "neuron = Neuron(inputs)\n",
        "output = neuron.activate()\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcaOJ9fWX1qv",
        "outputId": "3c8c6a9a-a948-4169-d9f9-8bceacfeb20f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.65832214]\n",
            "[1.65832214]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's drive to loss function** "
      ],
      "metadata": {
        "id": "F5UQ6C60a2z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, inputs,target):\n",
        "        self.input=inputs\n",
        "        self.target=target\n",
        "        self.weights = np.random.random(len(inputs))\n",
        "        self.bias = np.random.random(1)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    \n",
        "    def Relu(self,x):\n",
        "        if x>0:\n",
        "          return x\n",
        "        else:\n",
        "          return 0 \n",
        "    def mse_loss(self):\n",
        "        loss = (self.activation - self.target) ** 2\n",
        "        return loss\n",
        "    \n",
        "    def activate(self):\n",
        "        \n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = np.dot(self.input, self.weights) + self.bias\n",
        "        print(weighted_sum)\n",
        "        \n",
        "        # Apply the activation function (in this case, sigmoid)\n",
        "        self.activation = self.sigmoid(weighted_sum)\n",
        "        \n",
        "        \n",
        "        return self.activation"
      ],
      "metadata": {
        "id": "2SmgZqUwX95X"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [0.5, 0.7, 2]\n",
        "target=.8\n",
        "neuron = Neuron(inputs,target)\n",
        "output = neuron.activate()\n",
        "loss=neuron.mse_loss()\n",
        "print(output)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FctsFj8hdBK2",
        "outputId": "ada371f7-e13c-42b0-9a0e-167bae8b7bca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.61500655]\n",
            "0.9318211535937547\n",
            "0.017376816534788262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use another activation function named binary cross entropy.Remember it mainly used for binary classification .So the label should be 0 or 1** "
      ],
      "metadata": {
        "id": "ElDFC2nWenXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, inputs,target):\n",
        "        self.input=inputs\n",
        "        self.target=target\n",
        "        self.weights = np.random.random(len(inputs))\n",
        "        self.bias = np.random.random(1)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    \n",
        "    def Relu(self,x):\n",
        "        if x>0:\n",
        "          return x\n",
        "        else:\n",
        "          return 0 \n",
        "    def mse_loss(self):\n",
        "        loss = (self.activation - self.target) ** 2\n",
        "        return loss\n",
        "    def crossentropy_loss(self):  \n",
        "        loss = -(target * np.log(self.activation) + (1 - target) * np.log(1 - self.activation))\n",
        "        return loss\n",
        "    def activate(self):\n",
        "        \n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = np.dot(self.input, self.weights) + self.bias\n",
        "        print(weighted_sum)\n",
        "        \n",
        "        # Apply the activation function (in this case, sigmoid)\n",
        "        self.activation = self.sigmoid(weighted_sum)\n",
        "        \n",
        "        \n",
        "        return self.activation"
      ],
      "metadata": {
        "id": "uOTjUZVXdDVU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [0.5, 0.7, 2]\n",
        "target=1\n",
        "neuron = Neuron(inputs,target)\n",
        "output = neuron.activate()\n",
        "loss=neuron.crossentropy_loss()\n",
        "print(output)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7aIgU7Geiiy",
        "outputId": "f1ee7c32-8dfd-45ce-eb5c-18a831ae3670"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.22715796]\n",
            "0.7733207628654225\n",
            "0.2570613580374056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**multilayer perception**"
      ],
      "metadata": {
        "id": "mJaFZs7jvki9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize weights and biases for hidden layers\n",
        "        prev_size = input_size\n",
        "\n",
        "        for size in hidden_sizes:\n",
        "            layer_weights = np.random.randn(prev_size, size)\n",
        "            print(layer_weights.shape)\n",
        "            layer_biases = np.zeros(size)\n",
        "            self.weights.append(layer_weights)\n",
        "            self.biases.append(layer_biases)\n",
        "            prev_size = size\n",
        "        \n",
        "        # Initialize weights and biases for output layer\n",
        "        output_weights = np.random.randn(prev_size, output_size)\n",
        "        print(output_weights.shape)\n",
        "        output_biases = np.zeros(output_size)\n",
        "        self.weights.append(output_weights)\n",
        "        self.biases.append(output_biases)\n",
        "    \n",
        "    def activate(self, inputs):\n",
        "        layer_outputs = inputs\n",
        "        for layer_weights, layer_biases in zip(self.weights, self.biases):\n",
        "            weighted_sum = np.dot(layer_outputs, layer_weights) + layer_biases\n",
        "            layer_outputs = self.sigmoid(weighted_sum)\n",
        "        return layer_outputs\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "2BXEjEuafAQI"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural network with 2 input neurons, 3 neurons in the first hidden layer,\n",
        "# 4 neurons in the second hidden layer, and 1 output neuron\n",
        "network = NeuralNetwork(2, [3, 4], 2)\n",
        "\n",
        "# Provide inputs for prediction\n",
        "inputs = np.array([0.5, 0.3])\n",
        "\n",
        "# Perform forward propagation and obtain the network's prediction\n",
        "prediction = network.activate(inputs)\n",
        "\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHeqyrU4vtSm",
        "outputId": "f8c8d10b-877e-4592-8720-b2318e87f650"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.37700513 0.58207683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**build a model with 5 hidden layers where input size is 4 and 1 output node. Model should use MSE loss for error calculation.Also use ReLu to all the hidden layer and sigmoid for output layer**"
      ],
      "metadata": {
        "id": "ZmfDJQzTxDIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetworkU:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize weights and biases for hidden layers\n",
        "        prev_size = input_size\n",
        "        for size in hidden_sizes:\n",
        "            layer_weights = np.random.randn(prev_size, size)\n",
        "            print(layer_weights.shape)\n",
        "            layer_biases = np.zeros(size)\n",
        "            self.weights.append(layer_weights)\n",
        "            self.biases.append(layer_biases)\n",
        "            prev_size = size\n",
        "        \n",
        "        # Initialize weights and biases for output layer\n",
        "        output_weights = np.random.randn(prev_size, output_size)\n",
        "        print(output_weights.shape)\n",
        "        output_biases = np.zeros(output_size)\n",
        "        self.weights.append(output_weights)\n",
        "        self.biases.append(output_biases)\n",
        "        \n",
        "    \n",
        "    def activate(self, inputs):\n",
        "        layer_outputs = inputs\n",
        "        for layer_weights, layer_biases in zip(self.weights, self.biases):\n",
        "            weighted_sum = np.dot(layer_outputs, layer_weights) + layer_biases\n",
        "            layer_outputs = self.sigmoid(weighted_sum)\n",
        "        self.layer_outputs=layer_outputs\n",
        "        return self.layer_outputs\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def calculate_loss(self,target):\n",
        "        \n",
        "        loss = np.mean((self.layer_outputs - target) ** 2)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "_eYTlQxXvuDm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = NeuralNetworkU(4, [3, 4,5,6,3], 1)\n",
        "\n",
        "\n",
        "\n",
        "input = np.array([0.5, 0.3,0.4,.07])\n",
        "target = np.array([0.8])\n",
        "output = network.activate(input)\n",
        "loss = network.calculate_loss(target)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wlhuhiWv4mC",
        "outputId": "202f7355-cf71-4f5d-d14a-32c27f791e9c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 3)\n",
            "(3, 4)\n",
            "(4, 5)\n",
            "(5, 6)\n",
            "(6, 3)\n",
            "(3, 1)\n",
            "0.02782057395053311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we use it in image classification**"
      ],
      "metadata": {
        "id": "pkrLs1Nrm0Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = NeuralNetworkU(4, [3, 4,5,6,3], 1)\n",
        "input_image=np.random.randn(2,2)\n",
        "input = input_image.reshape(4)\n",
        "target = np.array([0.8])\n",
        "output = network.activate(input)\n",
        "loss = network.calculate_loss(target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni4cM1jrmPUT",
        "outputId": "09ebb83f-4c71-4457-eecb-527afb9e8112"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 3)\n",
            "(3, 4)\n",
            "(4, 5)\n",
            "(5, 6)\n",
            "(6, 3)\n",
            "(3, 1)\n",
            "0.04707403215070743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFhjjDSymqxR",
        "outputId": "5c0b20ad-a2fb-4690-c6a7-d8dced437305"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.9318528 ,  1.67871106,  1.26337953,  0.80405211])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's build a real model and learn about automatic gradient**"
      ],
      "metadata": {
        "id": "NRW5fm8yyh5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tabular dataset"
      ],
      "metadata": {
        "id": "Gw-tvaTQomUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "print(df.head(5))\n",
        "df['target'] = data.target\n",
        "print(df['target'])\n",
        "# Separate the features and labels\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heOdgt7pn7u5",
        "outputId": "ac63fd23-aa4c-4743-c445-faae119463aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                 0.07871  ...         25.38          17.33           184.60   \n",
            "1                 0.05667  ...         24.99          23.41           158.80   \n",
            "2                 0.05999  ...         23.57          25.53           152.50   \n",
            "3                 0.09744  ...         14.91          26.50            98.87   \n",
            "4                 0.05883  ...         22.54          16.67           152.20   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0      2019.0            0.1622             0.6656           0.7119   \n",
            "1      1956.0            0.1238             0.1866           0.2416   \n",
            "2      1709.0            0.1444             0.4245           0.4504   \n",
            "3       567.7            0.2098             0.8663           0.6869   \n",
            "4      1575.0            0.1374             0.2050           0.4000   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0                0.2654          0.4601                  0.11890  \n",
            "1                0.1860          0.2750                  0.08902  \n",
            "2                0.2430          0.3613                  0.08758  \n",
            "3                0.2575          0.6638                  0.17300  \n",
            "4                0.1625          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "564    0\n",
            "565    0\n",
            "566    0\n",
            "567    0\n",
            "568    1\n",
            "Name: target, Length: 569, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    \n",
        "    # Mini-batch training\n",
        "    for batch in range(0, len(X_train), batch_size):\n",
        "        X_batch = X_train[batch:batch+batch_size]\n",
        "        y_batch = y_train[batch:batch+batch_size]\n",
        "        \n",
        "        # Perform forward pass and compute gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(X_batch, training=True)\n",
        "            \n",
        "            # Reshape the labels\n",
        "            y_batch = tf.reshape(y_batch, (-1, 1))\n",
        "            \n",
        "            loss_value = tf.keras.losses.binary_crossentropy(y_batch, logits)\n",
        "        \n",
        "        # Retrieve model variables\n",
        "        trainable_vars = model.trainable_variables\n",
        "        \n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss_value, trainable_vars)\n",
        "        \n",
        "        # Update model weights\n",
        "        model.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    \n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Validation loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    print(\"--------------------\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CpJVW7szZBR",
        "outputId": "6eac42c5-e50c-4c55-cad4-12d45986b038"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.9649\n",
            "Validation loss: 0.2958, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9737\n",
            "Validation loss: 0.1568, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9737\n",
            "Validation loss: 0.1047, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9825\n",
            "Validation loss: 0.0834, Accuracy: 0.9825\n",
            "--------------------\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9737\n",
            "Validation loss: 0.0743, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9737\n",
            "Validation loss: 0.0695, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9737\n",
            "Validation loss: 0.0665, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9737\n",
            "Validation loss: 0.0648, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9737\n",
            "Validation loss: 0.0641, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9737\n",
            "Validation loss: 0.0638, Accuracy: 0.9737\n",
            "--------------------\n",
            "4/4 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Separate the features and labels\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train,epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(loss, accuracy )"
      ],
      "metadata": {
        "id": "kKjcnRyg5uAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train with the pseudo test dataset**"
      ],
      "metadata": {
        "id": "pg6O7uH350lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Separate the features and labels\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train,validation_data=(X_val,y_val),epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(loss, accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCdrFQM-yicH",
        "outputId": "a62784be-a9ea-474b-88b2-632e9c7edb9d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 1s 20ms/step - loss: 0.6160 - accuracy: 0.6978 - val_loss: 44.0283 - val_accuracy: 0.3956\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3352 - accuracy: 0.9451 - val_loss: 92.1634 - val_accuracy: 0.3956\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2011 - accuracy: 0.9588 - val_loss: 138.9603 - val_accuracy: 0.3956\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9643 - val_loss: 191.3602 - val_accuracy: 0.3956\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1045 - accuracy: 0.9835 - val_loss: 249.2653 - val_accuracy: 0.3956\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9863 - val_loss: 291.8917 - val_accuracy: 0.3956\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9863 - val_loss: 322.0127 - val_accuracy: 0.3956\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0579 - accuracy: 0.9890 - val_loss: 346.6037 - val_accuracy: 0.3956\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9918 - val_loss: 374.9762 - val_accuracy: 0.3956\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9918 - val_loss: 393.1537 - val_accuracy: 0.3956\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9561\n",
            "0.07704398781061172 0.9561403393745422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Implement\n",
        "1.   Dynamic learning rate\n",
        "2.   Save model \n",
        "3.   Early stopping \n",
        "4.   Drop-Out\n",
        "\n"
      ],
      "metadata": {
        "id": "0UHYBVqp6DsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Load the Breast Cancer dataset from sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Separate the features and labels\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model with dropout regularization\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=64,input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the learning rate reduction callback\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',   # Monitor validation loss for learning rate reduction\n",
        "    factor=0.1,            # Reduce learning rate by a factor of 0.1\n",
        "    patience=5,            # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    min_lr=1e-6            # Minimum learning rate\n",
        ")\n",
        "# Define the model checkpoint callback to save the best model\n",
        "checkpoint_callback = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Define the early stopping callback to stop training if validation loss does not improve\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model with the learning rate reduction callback\n",
        "model.fit(X_train, y_train,validation_data=(X_val,y_val),epochs=10, batch_size=32,callbacks=[lr_callback,checkpoint_callback,early_stopping_callback])\n",
        "\n",
        "# Evaluate the model\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n",
        "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
        "\n",
        "print(loss, accuracy )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s-Vq7vf4f3k",
        "outputId": "68319510-d319-4bd4-ca65-7dc3778df442"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 2s 68ms/step - loss: 0.7098 - accuracy: 0.5742 - val_loss: 22.8123 - val_accuracy: 0.6044 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.4461 - accuracy: 0.8187 - val_loss: 23.5115 - val_accuracy: 0.3956 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 0.3073 - accuracy: 0.9093 - val_loss: 51.9431 - val_accuracy: 0.3956 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 0.2249 - accuracy: 0.9313 - val_loss: 73.7355 - val_accuracy: 0.3956 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.2225 - accuracy: 0.9286 - val_loss: 92.0137 - val_accuracy: 0.3956 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.1986 - accuracy: 0.9423 - val_loss: 108.6324 - val_accuracy: 0.3956 - lr: 0.0010\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.9211\n",
            "0.43310025334358215 0.9210526347160339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPZaVQTAyjbg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkdlHu750jhC",
        "outputId": "89503dbd-c4d7-45fb-ea22-37f0248c154b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 14ms/step - loss: 0.3168 - accuracy: 0.9649\n",
            "Validation loss: 0.3168, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.9737\n",
            "Validation loss: 0.1651, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1012 - accuracy: 0.9737\n",
            "Validation loss: 0.1012, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9737\n",
            "Validation loss: 0.0779, Accuracy: 0.9737\n",
            "--------------------\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9649\n",
            "Validation loss: 0.0695, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0657 - accuracy: 0.9649\n",
            "Validation loss: 0.0657, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9649\n",
            "Validation loss: 0.0634, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9649\n",
            "Validation loss: 0.0620, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9649\n",
            "Validation loss: 0.0613, Accuracy: 0.9649\n",
            "--------------------\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9737\n",
            "Validation loss: 0.0609, Accuracy: 0.9737\n",
            "--------------------\n",
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8OjmEip1Thg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}