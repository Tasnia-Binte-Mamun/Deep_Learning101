{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"n_filters = 64\ndilation_depth = 8\nactivation = 'softmax'\nscale_ratio = 1\nkernel_size = 2\npool_size_1 = 4\npool_size_2 = 8\ntraget=5\nbatch_size = 32\ninput_shape=(16000,1)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:09:39.780613Z","iopub.execute_input":"2023-07-17T19:09:39.781369Z","iopub.status.idle":"2023-07-17T19:09:39.788409Z","shell.execute_reply.started":"2023-07-17T19:09:39.781330Z","shell.execute_reply":"2023-07-17T19:09:39.785628Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import glob\nimport librosa\nimport numpy as np\n\ndata1 = []\nlabels1 = []\n\nCAS = glob.glob('/kaggle/input/primaryicbhi2017/mixed_5s/Asthma/*.wav')\nCASandDAS = glob.glob('/kaggle/input/primaryicbhi2017/mixed_5s/Asthma/*.wav')\nDAS = glob.glob('/kaggle/input/primaryicbhi2017/mixed_5s/COPD/*.wav')\nNormal = glob.glob('/kaggle/input/primaryicbhi2017/mixed_5s/Healthy/*.wav')\nPoor_Quality = glob.glob('/kaggle/input/primaryicbhi2017/mixed_5s/Pneumonia/*.wav')\n\n\nSAMPLE_RATE=4000\n\nnsample=16000\nfor file_path in CAS:   \n    #print(file_path)\n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n    signal=np.array(signal[0:nsample])\n#     print(signal.shape)\n    data1.append(signal)\n    labels1.append(0)\n\nfor file_path in CASandDAS:   \n   # print(file_path)\n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n    signal=np.array(signal[0:nsample])\n#     print(signal.shape)\n    data1.append(signal)\n    labels1.append(1)\nfor file_path in DAS:   \n    #print(file_path)\n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n\n    signal=np.array(signal[0:nsample])\n#     print(signal.shape)\n    data1.append(signal)\n    labels1.append(2)\n\nfor file_path in Normal:   \n   # print(file_path)\n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n\n    \n    signal=np.array(signal[0:nsample])\n#     print(signal.shape)\n    data1.append(signal)\n    labels1.append(3)\nfor file_path in Poor_Quality:   \n    #print(file_path)\n    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n    \n    signal=np.array(signal[0:nsample])\n    if signal.shape==(nsample,):\n#     print(signal.shape)\n        data1.append(signal)\n        labels1.append(4)\n    else:\n        continue\nX_train= np.array(data1)\ny_train= np.array(labels1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:09:39.790171Z","iopub.execute_input":"2023-07-17T19:09:39.790560Z","iopub.status.idle":"2023-07-17T19:10:24.753908Z","shell.execute_reply.started":"2023-07-17T19:09:39.790500Z","shell.execute_reply":"2023-07-17T19:10:24.752768Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"np.unique(y_train,return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:10:24.756306Z","iopub.execute_input":"2023-07-17T19:10:24.757097Z","iopub.status.idle":"2023-07-17T19:10:24.768281Z","shell.execute_reply.started":"2023-07-17T19:10:24.757058Z","shell.execute_reply":"2023-07-17T19:10:24.767138Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(array([0, 1, 2, 3, 4]), array([431, 431, 805, 660, 260]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Make the residual block of the WaveNet","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv1D, Input, Activation, AveragePooling1D, Add, Multiply, GlobalAveragePooling1D\nfrom keras.models import Model\ndef residual_block(x, i):\n    tanh_out = Conv1D(n_filters, \n                      kernel_size, \n                      dilation_rate = kernel_size**i, \n                      padding='causal',\n                      activation='tanh'\n                      )(x)\n    sigm_out = Conv1D(n_filters, \n                      kernel_size, \n                      dilation_rate = kernel_size**i, \n                      padding='causal', \n                      activation='sigmoid'\n                      )(x)\n    z = Multiply()([tanh_out, sigm_out])\n    skip = Conv1D(n_filters, 1)(z)\n    res = Add()([skip, x])\n    return res, skip\nx = Input(shape=input_shape)\nskip_connections = []\nout = Conv1D(n_filters, 2, dilation_rate=1, padding='causal')(x)\nfor i in range(1, dilation_depth + 1):\n    out, skip = residual_block(out,i)\n    skip_connections.append(skip)\nout = Add()(skip_connections)\nout = Activation('relu')(out)\nout = Conv1D(traget,1, padding='same', activation='relu')(out)\nout = Conv1D(traget,1 , padding='same', activation='relu')(out)\nout = GlobalAveragePooling1D()(out)\nout = Activation(activation)(out)\n\nmodel = Model(x, out)  \nmodel.compile(optimizer='adam', \n                          loss='sparse_categorical_crossentropy', \n                          metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:10:24.770089Z","iopub.execute_input":"2023-07-17T19:10:24.770455Z","iopub.status.idle":"2023-07-17T19:10:28.091322Z","shell.execute_reply.started":"2023-07-17T19:10:24.770422Z","shell.execute_reply":"2023-07-17T19:10:28.090560Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 16000, 1)]   0           []                               \n                                                                                                  \n conv1d (Conv1D)                (None, 16000, 64)    192         ['input_1[0][0]']                \n                                                                                                  \n conv1d_1 (Conv1D)              (None, 16000, 64)    8256        ['conv1d[0][0]']                 \n                                                                                                  \n conv1d_2 (Conv1D)              (None, 16000, 64)    8256        ['conv1d[0][0]']                 \n                                                                                                  \n multiply (Multiply)            (None, 16000, 64)    0           ['conv1d_1[0][0]',               \n                                                                  'conv1d_2[0][0]']               \n                                                                                                  \n conv1d_3 (Conv1D)              (None, 16000, 64)    4160        ['multiply[0][0]']               \n                                                                                                  \n add (Add)                      (None, 16000, 64)    0           ['conv1d_3[0][0]',               \n                                                                  'conv1d[0][0]']                 \n                                                                                                  \n conv1d_4 (Conv1D)              (None, 16000, 64)    8256        ['add[0][0]']                    \n                                                                                                  \n conv1d_5 (Conv1D)              (None, 16000, 64)    8256        ['add[0][0]']                    \n                                                                                                  \n multiply_1 (Multiply)          (None, 16000, 64)    0           ['conv1d_4[0][0]',               \n                                                                  'conv1d_5[0][0]']               \n                                                                                                  \n conv1d_6 (Conv1D)              (None, 16000, 64)    4160        ['multiply_1[0][0]']             \n                                                                                                  \n add_1 (Add)                    (None, 16000, 64)    0           ['conv1d_6[0][0]',               \n                                                                  'add[0][0]']                    \n                                                                                                  \n conv1d_7 (Conv1D)              (None, 16000, 64)    8256        ['add_1[0][0]']                  \n                                                                                                  \n conv1d_8 (Conv1D)              (None, 16000, 64)    8256        ['add_1[0][0]']                  \n                                                                                                  \n multiply_2 (Multiply)          (None, 16000, 64)    0           ['conv1d_7[0][0]',               \n                                                                  'conv1d_8[0][0]']               \n                                                                                                  \n conv1d_9 (Conv1D)              (None, 16000, 64)    4160        ['multiply_2[0][0]']             \n                                                                                                  \n add_2 (Add)                    (None, 16000, 64)    0           ['conv1d_9[0][0]',               \n                                                                  'add_1[0][0]']                  \n                                                                                                  \n conv1d_10 (Conv1D)             (None, 16000, 64)    8256        ['add_2[0][0]']                  \n                                                                                                  \n conv1d_11 (Conv1D)             (None, 16000, 64)    8256        ['add_2[0][0]']                  \n                                                                                                  \n multiply_3 (Multiply)          (None, 16000, 64)    0           ['conv1d_10[0][0]',              \n                                                                  'conv1d_11[0][0]']              \n                                                                                                  \n conv1d_12 (Conv1D)             (None, 16000, 64)    4160        ['multiply_3[0][0]']             \n                                                                                                  \n add_3 (Add)                    (None, 16000, 64)    0           ['conv1d_12[0][0]',              \n                                                                  'add_2[0][0]']                  \n                                                                                                  \n conv1d_13 (Conv1D)             (None, 16000, 64)    8256        ['add_3[0][0]']                  \n                                                                                                  \n conv1d_14 (Conv1D)             (None, 16000, 64)    8256        ['add_3[0][0]']                  \n                                                                                                  \n multiply_4 (Multiply)          (None, 16000, 64)    0           ['conv1d_13[0][0]',              \n                                                                  'conv1d_14[0][0]']              \n                                                                                                  \n conv1d_15 (Conv1D)             (None, 16000, 64)    4160        ['multiply_4[0][0]']             \n                                                                                                  \n add_4 (Add)                    (None, 16000, 64)    0           ['conv1d_15[0][0]',              \n                                                                  'add_3[0][0]']                  \n                                                                                                  \n conv1d_16 (Conv1D)             (None, 16000, 64)    8256        ['add_4[0][0]']                  \n                                                                                                  \n conv1d_17 (Conv1D)             (None, 16000, 64)    8256        ['add_4[0][0]']                  \n                                                                                                  \n multiply_5 (Multiply)          (None, 16000, 64)    0           ['conv1d_16[0][0]',              \n                                                                  'conv1d_17[0][0]']              \n                                                                                                  \n conv1d_18 (Conv1D)             (None, 16000, 64)    4160        ['multiply_5[0][0]']             \n                                                                                                  \n add_5 (Add)                    (None, 16000, 64)    0           ['conv1d_18[0][0]',              \n                                                                  'add_4[0][0]']                  \n                                                                                                  \n conv1d_19 (Conv1D)             (None, 16000, 64)    8256        ['add_5[0][0]']                  \n                                                                                                  \n conv1d_20 (Conv1D)             (None, 16000, 64)    8256        ['add_5[0][0]']                  \n                                                                                                  \n multiply_6 (Multiply)          (None, 16000, 64)    0           ['conv1d_19[0][0]',              \n                                                                  'conv1d_20[0][0]']              \n                                                                                                  \n conv1d_21 (Conv1D)             (None, 16000, 64)    4160        ['multiply_6[0][0]']             \n                                                                                                  \n add_6 (Add)                    (None, 16000, 64)    0           ['conv1d_21[0][0]',              \n                                                                  'add_5[0][0]']                  \n                                                                                                  \n conv1d_22 (Conv1D)             (None, 16000, 64)    8256        ['add_6[0][0]']                  \n                                                                                                  \n conv1d_23 (Conv1D)             (None, 16000, 64)    8256        ['add_6[0][0]']                  \n                                                                                                  \n multiply_7 (Multiply)          (None, 16000, 64)    0           ['conv1d_22[0][0]',              \n                                                                  'conv1d_23[0][0]']              \n                                                                                                  \n conv1d_24 (Conv1D)             (None, 16000, 64)    4160        ['multiply_7[0][0]']             \n                                                                                                  \n add_8 (Add)                    (None, 16000, 64)    0           ['conv1d_3[0][0]',               \n                                                                  'conv1d_6[0][0]',               \n                                                                  'conv1d_9[0][0]',               \n                                                                  'conv1d_12[0][0]',              \n                                                                  'conv1d_15[0][0]',              \n                                                                  'conv1d_18[0][0]',              \n                                                                  'conv1d_21[0][0]',              \n                                                                  'conv1d_24[0][0]']              \n                                                                                                  \n activation (Activation)        (None, 16000, 64)    0           ['add_8[0][0]']                  \n                                                                                                  \n conv1d_25 (Conv1D)             (None, 16000, 5)     325         ['activation[0][0]']             \n                                                                                                  \n conv1d_26 (Conv1D)             (None, 16000, 5)     30          ['conv1d_25[0][0]']              \n                                                                                                  \n global_average_pooling1d (Glob  (None, 5)           0           ['conv1d_26[0][0]']              \n alAveragePooling1D)                                                                              \n                                                                                                  \n activation_1 (Activation)      (None, 5)            0           ['global_average_pooling1d[0][0]'\n                                                                 ]                                \n                                                                                                  \n==================================================================================================\nTotal params: 165,923\nTrainable params: 165,923\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(y_train),\n                                        y = y_train                                                   \n                                    )\nclass_weights = dict(zip(np.unique(y_train), class_weights))\nclass_weights\n\ncheckpoint_path = 'best.h5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    verbose=1\n)\n\nimport numpy as np\n# from focal_loss import SparseCategoricalFocalLoss\nfrom sklearn.model_selection import train_test_split\n\n# Train the model\nbatch_size = 16\nepochs = 100\n\nmodel.fit(X_train, y_train,callbacks=[checkpoint],validation_split=.2,epochs=epochs ,class_weight=class_weights, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:12:49.051940Z","iopub.execute_input":"2023-07-17T19:12:49.052310Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n130/130 [==============================] - ETA: 0s - loss: 1.1144 - accuracy: 0.5268\nEpoch 1: val_accuracy improved from -inf to 0.03668, saving model to best.h5\n130/130 [==============================] - 25s 195ms/step - loss: 1.1144 - accuracy: 0.5268 - val_loss: 2.6308 - val_accuracy: 0.0367\nEpoch 2/100\n130/130 [==============================] - ETA: 0s - loss: 1.0929 - accuracy: 0.5317\nEpoch 2: val_accuracy did not improve from 0.03668\n130/130 [==============================] - 24s 186ms/step - loss: 1.0929 - accuracy: 0.5317 - val_loss: 2.7043 - val_accuracy: 0.0193\nEpoch 3/100\n130/130 [==============================] - ETA: 0s - loss: 1.0844 - accuracy: 0.5423\nEpoch 3: val_accuracy improved from 0.03668 to 0.04633, saving model to best.h5\n130/130 [==============================] - 24s 183ms/step - loss: 1.0844 - accuracy: 0.5423 - val_loss: 3.1211 - val_accuracy: 0.0463\nEpoch 4/100\n130/130 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.5423\nEpoch 4: val_accuracy improved from 0.04633 to 0.07143, saving model to best.h5\n130/130 [==============================] - 24s 187ms/step - loss: 1.0755 - accuracy: 0.5423 - val_loss: 2.4879 - val_accuracy: 0.0714\nEpoch 5/100\n130/130 [==============================] - ETA: 0s - loss: 1.0768 - accuracy: 0.5404\nEpoch 5: val_accuracy did not improve from 0.07143\n130/130 [==============================] - 24s 182ms/step - loss: 1.0768 - accuracy: 0.5404 - val_loss: 2.6602 - val_accuracy: 0.0541\nEpoch 6/100\n130/130 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.5384\nEpoch 6: val_accuracy did not improve from 0.07143\n130/130 [==============================] - 24s 186ms/step - loss: 1.0778 - accuracy: 0.5384 - val_loss: 2.7928 - val_accuracy: 0.0309\nEpoch 7/100\n130/130 [==============================] - ETA: 0s - loss: 1.0724 - accuracy: 0.5404\nEpoch 7: val_accuracy did not improve from 0.07143\n130/130 [==============================] - 24s 186ms/step - loss: 1.0724 - accuracy: 0.5404 - val_loss: 3.2477 - val_accuracy: 0.0444\nEpoch 8/100\n130/130 [==============================] - ETA: 0s - loss: 1.0603 - accuracy: 0.5505\nEpoch 8: val_accuracy did not improve from 0.07143\n130/130 [==============================] - 24s 182ms/step - loss: 1.0603 - accuracy: 0.5505 - val_loss: 3.5130 - val_accuracy: 0.0328\nEpoch 9/100\n130/130 [==============================] - ETA: 0s - loss: 1.0552 - accuracy: 0.5568\nEpoch 9: val_accuracy improved from 0.07143 to 0.13320, saving model to best.h5\n130/130 [==============================] - 24s 183ms/step - loss: 1.0552 - accuracy: 0.5568 - val_loss: 2.8059 - val_accuracy: 0.1332\nEpoch 10/100\n130/130 [==============================] - ETA: 0s - loss: 1.0600 - accuracy: 0.5529\nEpoch 10: val_accuracy did not improve from 0.13320\n130/130 [==============================] - 24s 186ms/step - loss: 1.0600 - accuracy: 0.5529 - val_loss: 3.6794 - val_accuracy: 0.0618\nEpoch 11/100\n130/130 [==============================] - ETA: 0s - loss: 1.0515 - accuracy: 0.5631\nEpoch 11: val_accuracy did not improve from 0.13320\n130/130 [==============================] - 24s 186ms/step - loss: 1.0515 - accuracy: 0.5631 - val_loss: 3.1912 - val_accuracy: 0.1293\nEpoch 12/100\n130/130 [==============================] - ETA: 0s - loss: 1.0533 - accuracy: 0.5563\nEpoch 12: val_accuracy improved from 0.13320 to 0.13900, saving model to best.h5\n130/130 [==============================] - 24s 183ms/step - loss: 1.0533 - accuracy: 0.5563 - val_loss: 3.4888 - val_accuracy: 0.1390\nEpoch 13/100\n130/130 [==============================] - ETA: 0s - loss: 1.0449 - accuracy: 0.5636\nEpoch 13: val_accuracy did not improve from 0.13900\n130/130 [==============================] - 24s 186ms/step - loss: 1.0449 - accuracy: 0.5636 - val_loss: 3.1431 - val_accuracy: 0.0734\nEpoch 14/100\n130/130 [==============================] - ETA: 0s - loss: 1.0459 - accuracy: 0.5645\nEpoch 14: val_accuracy did not improve from 0.13900\n130/130 [==============================] - 24s 186ms/step - loss: 1.0459 - accuracy: 0.5645 - val_loss: 3.1816 - val_accuracy: 0.1197\nEpoch 15/100\n130/130 [==============================] - ETA: 0s - loss: 1.0476 - accuracy: 0.5660\nEpoch 15: val_accuracy improved from 0.13900 to 0.15637, saving model to best.h5\n130/130 [==============================] - 24s 187ms/step - loss: 1.0476 - accuracy: 0.5660 - val_loss: 2.9761 - val_accuracy: 0.1564\nEpoch 16/100\n130/130 [==============================] - ETA: 0s - loss: 1.0383 - accuracy: 0.5679\nEpoch 16: val_accuracy did not improve from 0.15637\n130/130 [==============================] - 24s 182ms/step - loss: 1.0383 - accuracy: 0.5679 - val_loss: 2.9265 - val_accuracy: 0.1120\nEpoch 17/100\n130/130 [==============================] - ETA: 0s - loss: 1.0286 - accuracy: 0.5805\nEpoch 17: val_accuracy did not improve from 0.15637\n130/130 [==============================] - 24s 186ms/step - loss: 1.0286 - accuracy: 0.5805 - val_loss: 4.3912 - val_accuracy: 0.1564\nEpoch 18/100\n130/130 [==============================] - ETA: 0s - loss: 1.0236 - accuracy: 0.5819\nEpoch 18: val_accuracy improved from 0.15637 to 0.17954, saving model to best.h5\n130/130 [==============================] - 24s 187ms/step - loss: 1.0236 - accuracy: 0.5819 - val_loss: 3.7813 - val_accuracy: 0.1795\nEpoch 19/100\n130/130 [==============================] - ETA: 0s - loss: 1.0149 - accuracy: 0.5897\nEpoch 19: val_accuracy improved from 0.17954 to 0.18533, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 1.0149 - accuracy: 0.5897 - val_loss: 4.6311 - val_accuracy: 0.1853\nEpoch 20/100\n130/130 [==============================] - ETA: 0s - loss: 1.0195 - accuracy: 0.5863\nEpoch 20: val_accuracy did not improve from 0.18533\n130/130 [==============================] - 24s 185ms/step - loss: 1.0195 - accuracy: 0.5863 - val_loss: 4.4206 - val_accuracy: 0.1236\nEpoch 21/100\n130/130 [==============================] - ETA: 0s - loss: 1.0213 - accuracy: 0.5858\nEpoch 21: val_accuracy improved from 0.18533 to 0.21042, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 1.0213 - accuracy: 0.5858 - val_loss: 5.0957 - val_accuracy: 0.2104\nEpoch 22/100\n130/130 [==============================] - ETA: 0s - loss: 1.0058 - accuracy: 0.5964\nEpoch 22: val_accuracy did not improve from 0.21042\n130/130 [==============================] - 24s 185ms/step - loss: 1.0058 - accuracy: 0.5964 - val_loss: 5.1312 - val_accuracy: 0.1583\nEpoch 23/100\n130/130 [==============================] - ETA: 0s - loss: 1.0047 - accuracy: 0.5930\nEpoch 23: val_accuracy improved from 0.21042 to 0.21815, saving model to best.h5\n130/130 [==============================] - 24s 182ms/step - loss: 1.0047 - accuracy: 0.5930 - val_loss: 5.8027 - val_accuracy: 0.2181\nEpoch 24/100\n130/130 [==============================] - ETA: 0s - loss: 0.9993 - accuracy: 0.5984\nEpoch 24: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 181ms/step - loss: 0.9993 - accuracy: 0.5984 - val_loss: 4.5053 - val_accuracy: 0.1931\nEpoch 25/100\n130/130 [==============================] - ETA: 0s - loss: 1.0016 - accuracy: 0.6071\nEpoch 25: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 1.0016 - accuracy: 0.6071 - val_loss: 4.2210 - val_accuracy: 0.1988\nEpoch 26/100\n130/130 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.6066\nEpoch 26: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 181ms/step - loss: 0.9920 - accuracy: 0.6066 - val_loss: 5.4496 - val_accuracy: 0.2046\nEpoch 27/100\n130/130 [==============================] - ETA: 0s - loss: 0.9901 - accuracy: 0.6051\nEpoch 27: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 0.9901 - accuracy: 0.6051 - val_loss: 5.4185 - val_accuracy: 0.1815\nEpoch 28/100\n130/130 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.6017\nEpoch 28: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 181ms/step - loss: 0.9906 - accuracy: 0.6017 - val_loss: 4.2775 - val_accuracy: 0.2104\nEpoch 29/100\n130/130 [==============================] - ETA: 0s - loss: 0.9998 - accuracy: 0.6008\nEpoch 29: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 0.9998 - accuracy: 0.6008 - val_loss: 7.2014 - val_accuracy: 0.1506\nEpoch 30/100\n130/130 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.6148\nEpoch 30: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 0.9777 - accuracy: 0.6148 - val_loss: 6.2247 - val_accuracy: 0.1853\nEpoch 31/100\n130/130 [==============================] - ETA: 0s - loss: 0.9690 - accuracy: 0.6245\nEpoch 31: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 0.9690 - accuracy: 0.6245 - val_loss: 6.7994 - val_accuracy: 0.1622\nEpoch 32/100\n130/130 [==============================] - ETA: 0s - loss: 0.9693 - accuracy: 0.6274\nEpoch 32: val_accuracy did not improve from 0.21815\n130/130 [==============================] - 24s 185ms/step - loss: 0.9693 - accuracy: 0.6274 - val_loss: 7.0873 - val_accuracy: 0.1931\nEpoch 33/100\n130/130 [==============================] - ETA: 0s - loss: 0.9547 - accuracy: 0.6317\nEpoch 33: val_accuracy improved from 0.21815 to 0.23166, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.9547 - accuracy: 0.6317 - val_loss: 6.3753 - val_accuracy: 0.2317\nEpoch 34/100\n130/130 [==============================] - ETA: 0s - loss: 0.9500 - accuracy: 0.6375\nEpoch 34: val_accuracy did not improve from 0.23166\n130/130 [==============================] - 24s 181ms/step - loss: 0.9500 - accuracy: 0.6375 - val_loss: 7.2077 - val_accuracy: 0.1911\nEpoch 35/100\n130/130 [==============================] - ETA: 0s - loss: 0.9456 - accuracy: 0.6452\nEpoch 35: val_accuracy did not improve from 0.23166\n130/130 [==============================] - 24s 185ms/step - loss: 0.9456 - accuracy: 0.6452 - val_loss: 7.2430 - val_accuracy: 0.2278\nEpoch 36/100\n130/130 [==============================] - ETA: 0s - loss: 0.9453 - accuracy: 0.6399\nEpoch 36: val_accuracy improved from 0.23166 to 0.29151, saving model to best.h5\n130/130 [==============================] - 24s 182ms/step - loss: 0.9453 - accuracy: 0.6399 - val_loss: 7.3060 - val_accuracy: 0.2915\nEpoch 37/100\n130/130 [==============================] - ETA: 0s - loss: 0.9493 - accuracy: 0.6428\nEpoch 37: val_accuracy did not improve from 0.29151\n130/130 [==============================] - 24s 185ms/step - loss: 0.9493 - accuracy: 0.6428 - val_loss: 6.3790 - val_accuracy: 0.1969\nEpoch 38/100\n130/130 [==============================] - ETA: 0s - loss: 0.9375 - accuracy: 0.6491\nEpoch 38: val_accuracy did not improve from 0.29151\n130/130 [==============================] - 24s 185ms/step - loss: 0.9375 - accuracy: 0.6491 - val_loss: 6.3744 - val_accuracy: 0.2587\nEpoch 39/100\n130/130 [==============================] - ETA: 0s - loss: 0.9336 - accuracy: 0.6544\nEpoch 39: val_accuracy did not improve from 0.29151\n130/130 [==============================] - 24s 185ms/step - loss: 0.9336 - accuracy: 0.6544 - val_loss: 6.8306 - val_accuracy: 0.1911\nEpoch 40/100\n130/130 [==============================] - ETA: 0s - loss: 0.9316 - accuracy: 0.6568\nEpoch 40: val_accuracy did not improve from 0.29151\n130/130 [==============================] - 24s 185ms/step - loss: 0.9316 - accuracy: 0.6568 - val_loss: 5.7126 - val_accuracy: 0.1815\nEpoch 41/100\n130/130 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.6626\nEpoch 41: val_accuracy improved from 0.29151 to 0.31081, saving model to best.h5\n130/130 [==============================] - 24s 182ms/step - loss: 0.9231 - accuracy: 0.6626 - val_loss: 8.0439 - val_accuracy: 0.3108\nEpoch 42/100\n130/130 [==============================] - ETA: 0s - loss: 0.9021 - accuracy: 0.6810\nEpoch 42: val_accuracy improved from 0.31081 to 0.32239, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.9021 - accuracy: 0.6810 - val_loss: 9.3106 - val_accuracy: 0.3224\nEpoch 43/100\n130/130 [==============================] - ETA: 0s - loss: 0.8895 - accuracy: 0.6907\nEpoch 43: val_accuracy did not improve from 0.32239\n130/130 [==============================] - 23s 181ms/step - loss: 0.8895 - accuracy: 0.6907 - val_loss: 9.9579 - val_accuracy: 0.2625\nEpoch 44/100\n130/130 [==============================] - ETA: 0s - loss: 0.9162 - accuracy: 0.6771\nEpoch 44: val_accuracy improved from 0.32239 to 0.34942, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.9162 - accuracy: 0.6771 - val_loss: 8.4110 - val_accuracy: 0.3494\nEpoch 45/100\n130/130 [==============================] - ETA: 0s - loss: 0.8661 - accuracy: 0.7144\nEpoch 45: val_accuracy did not improve from 0.34942\n130/130 [==============================] - 24s 185ms/step - loss: 0.8661 - accuracy: 0.7144 - val_loss: 8.4069 - val_accuracy: 0.2896\nEpoch 46/100\n130/130 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.7134\nEpoch 46: val_accuracy did not improve from 0.34942\n130/130 [==============================] - 24s 181ms/step - loss: 0.8621 - accuracy: 0.7134 - val_loss: 8.0389 - val_accuracy: 0.3301\nEpoch 47/100\n130/130 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.6834\nEpoch 47: val_accuracy improved from 0.34942 to 0.35135, saving model to best.h5\n130/130 [==============================] - 24s 182ms/step - loss: 0.9089 - accuracy: 0.6834 - val_loss: 6.0430 - val_accuracy: 0.3514\nEpoch 48/100\n130/130 [==============================] - ETA: 0s - loss: 0.8526 - accuracy: 0.7231\nEpoch 48: val_accuracy improved from 0.35135 to 0.38417, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.8526 - accuracy: 0.7231 - val_loss: 9.1726 - val_accuracy: 0.3842\nEpoch 49/100\n130/130 [==============================] - ETA: 0s - loss: 0.8335 - accuracy: 0.7322\nEpoch 49: val_accuracy did not improve from 0.38417\n130/130 [==============================] - 23s 181ms/step - loss: 0.8335 - accuracy: 0.7322 - val_loss: 8.9272 - val_accuracy: 0.3822\nEpoch 50/100\n130/130 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.7400\nEpoch 50: val_accuracy improved from 0.38417 to 0.41120, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.8204 - accuracy: 0.7400 - val_loss: 11.2759 - val_accuracy: 0.4112\nEpoch 51/100\n130/130 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.7472\nEpoch 51: val_accuracy did not improve from 0.41120\n130/130 [==============================] - 24s 185ms/step - loss: 0.8085 - accuracy: 0.7472 - val_loss: 10.1070 - val_accuracy: 0.4073\nEpoch 52/100\n130/130 [==============================] - ETA: 0s - loss: 0.7998 - accuracy: 0.7516\nEpoch 52: val_accuracy did not improve from 0.41120\n130/130 [==============================] - 24s 185ms/step - loss: 0.7998 - accuracy: 0.7516 - val_loss: 9.3192 - val_accuracy: 0.3591\nEpoch 53/100\n130/130 [==============================] - ETA: 0s - loss: 0.8150 - accuracy: 0.7496\nEpoch 53: val_accuracy did not improve from 0.41120\n130/130 [==============================] - 23s 181ms/step - loss: 0.8150 - accuracy: 0.7496 - val_loss: 13.5375 - val_accuracy: 0.3012\nEpoch 54/100\n130/130 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.7400\nEpoch 54: val_accuracy did not improve from 0.41120\n130/130 [==============================] - 24s 185ms/step - loss: 0.8247 - accuracy: 0.7400 - val_loss: 15.1771 - val_accuracy: 0.3514\nEpoch 55/100\n130/130 [==============================] - ETA: 0s - loss: 0.8661 - accuracy: 0.7144\nEpoch 55: val_accuracy improved from 0.41120 to 0.41892, saving model to best.h5\n130/130 [==============================] - 24s 182ms/step - loss: 0.8661 - accuracy: 0.7144 - val_loss: 10.5598 - val_accuracy: 0.4189\nEpoch 56/100\n130/130 [==============================] - ETA: 0s - loss: 0.7817 - accuracy: 0.7579\nEpoch 56: val_accuracy improved from 0.41892 to 0.42664, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.7817 - accuracy: 0.7579 - val_loss: 10.6920 - val_accuracy: 0.4266\nEpoch 57/100\n130/130 [==============================] - ETA: 0s - loss: 0.7658 - accuracy: 0.7641\nEpoch 57: val_accuracy improved from 0.42664 to 0.43629, saving model to best.h5\n130/130 [==============================] - 24s 186ms/step - loss: 0.7658 - accuracy: 0.7641 - val_loss: 10.8574 - val_accuracy: 0.4363\nEpoch 58/100\n 79/130 [=================>............] - ETA: 8s - loss: 0.7676 - accuracy: 0.7674","output_type":"stream"}]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2023-07-17T19:12:32.870410Z","iopub.status.idle":"2023-07-17T19:12:32.871228Z","shell.execute_reply.started":"2023-07-17T19:12:32.870974Z","shell.execute_reply":"2023-07-17T19:12:32.870998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}